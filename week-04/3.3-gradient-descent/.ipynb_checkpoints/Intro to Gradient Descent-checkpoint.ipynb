{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending Into Neural Networks\n",
    "\n",
    "Over the next three weeks, I will be slowly introducing us to the internal mechanics of neural networks. Each week there will be a few lessons leading toward a deeper understanding of this topic. I will try to connect them to other material that we will be studying, but the primary purpose is to gradually learn this very complicated topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Techniques in General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a technique belonging to a larger family of iterative problem solvers. \n",
    "\n",
    "**Iteration** means to repeat a process with the aim of approaching a desired result. Each repetition of the process, also called an **iteration** is used as the starting point for the next iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Good News\n",
    "You have been doing this already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(3):\n",
    "    a += .1\n",
    "    print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One of the Oldest Iterative Techniques -  Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domain/Data:** This problem comes from the study of functions and typically will be applied to a polynomial function of a single variable. Here we will look at \n",
    "\n",
    "$$f(x) = x^3 - 6x^2 + 7x + 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lambda x: x**3 - 6*x**2 + 7*x + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement\n",
    "Given polynomial function of a single variable, we seek the **roots** of the function i.e. the values of $x$ for which the function evaluates to 0.\n",
    "\n",
    "$$x: f(x) = 0$$\n",
    "\n",
    "#### Solution Statement \n",
    "In order to solve this problem, we will use Newton's Method (or the Newton-Raphson Technique) to find successively better approximations to the roots.\n",
    "\n",
    "_Newton's Method_\n",
    "1. Visualize the function and find a guess that is \"close\" to the root\n",
    "2. Guess a value for the root, $x_0$.\n",
    "3. If the initial guess is close, we can find a better approximation\n",
    "\n",
    "   $$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\n",
    "   \n",
    "4. Repeat until a sufficiently accurate value is reached.\n",
    "\n",
    "   $$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle the derivative of our function, we will use the numerical approximation, the **difference quotient**\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "This should be sufficiently accurate for small enough values of $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h=1E-10):\n",
    "    return float((f(x + h) - f(x)) / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newtons_method_iteration(f, x):\n",
    "    return x - (f(x)/difference_quotient(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metric:** In order to evaluate the success of our root finding, we will examine the absolute error of the root approximation, where\n",
    "\n",
    "$$\\text{err}(x_{est}) = \\rvert f(x_{est}) \\rvert$$\n",
    "\n",
    "We can use this estimate because we expect the value of $f(x_{est}$ to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm_error = lambda f, x: np.abs(f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark:** We will set an error value of $10^{-10}$. When our error is less than this, we can stop iterating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "independent_values = np.arange(-1,5,0.1)\n",
    "dependent_values = f(independent_values)\n",
    "\n",
    "plt.plot(independent_values, \n",
    "         dependent_values, \n",
    "         label='$f(x)$')\n",
    "\n",
    "plt.axvline(c='black', linewidth=0.3)\n",
    "plt.axhline(c='black', linewidth=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the First Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(independent_values, \n",
    "         dependent_values, \n",
    "         label='$f(x)$')\n",
    "\n",
    "plt.xlim(-.5,.5)\n",
    "plt.ylim(-5,5)\n",
    "plt.axvline(c='black', linewidth=0.3)\n",
    "plt.axhline(c='black', linewidth=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Guess\n",
    "Try $x=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_1 = newtons_method_iteration(f, 0)\n",
    "x_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Five More Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_est = x_1\n",
    "for i in range(5):\n",
    "    x_est = newtons_method_iteration(f, x_est)\n",
    "    print x_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterations & Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_est = 0\n",
    "errors = []\n",
    "for i in range(6):\n",
    "    x_est = newtons_method_iteration(f, x_est)\n",
    "    error = nm_error(f, x_est)\n",
    "    errors.append(error)\n",
    "    print \"{:0.10f} {:0.10f} \".format(x_est, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(6), errors, label='error')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(independent_values, \n",
    "         dependent_values, \n",
    "         label='$f(x)$')\n",
    "\n",
    "x_est = 0\n",
    "for i in range(6):\n",
    "    plt.plot(x_est, f(x_est), '*')\n",
    "    x_est = newtons_method_iteration(f, x_est)\n",
    "\n",
    "plt.xlim(-.5,.1)\n",
    "plt.ylim(-5,5)\n",
    "plt.axvline(c='black', linewidth=0.3)\n",
    "plt.axhline(c='black', linewidth=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the Problem\n",
    "\n",
    "We found the first root, $x_1=-0.3300587396$.\n",
    "\n",
    "We also have a process for finding the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "independent_values = np.arange(-1,5,0.1)\n",
    "dependent_values = f(independent_values)\n",
    "\n",
    "plt.plot(independent_values, \n",
    "         dependent_values, \n",
    "         label='$f(x)$')\n",
    "\n",
    "plt.axvline(c='black', linewidth=0.3)\n",
    "plt.axhline(c='black', linewidth=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make an Initial Guess, the Iterate\n",
    "We will guess $x_2=2$ and $x_3=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_roots(function, x_initial, iterations=6):\n",
    "    x_estimate = x_initial\n",
    "    x_estimates = []\n",
    "    errors = []\n",
    "    for i in range(iterations):\n",
    "        x_estimate = newtons_method_iteration(f, x_estimate)\n",
    "        x_estimates.append(x_estimate)\n",
    "        \n",
    "        error = nm_error(f, x_estimate)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return x_estimates, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_two_ests, root_two_errs = find_roots(f, 2)\n",
    "for x_est, error in zip(root_two_ests, root_two_errs):\n",
    "    print \"{:0.10f} {:0.10f} \".format(x_est, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_three_ests, root_three_errs = find_roots(f, 4)\n",
    "for x_est, error in zip(root_three_ests, root_three_errs):\n",
    "    print \"{:0.10f} {:0.10f} \".format(x_est, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<!--  \n",
    "\n",
    "We found all three roots:\n",
    "\n",
    "\\begin{align}\n",
    "x_1 &= -0.3300587396\\\\\n",
    "x_2 &= 2.2016396757\\\\\n",
    "x_3 &= 4.1284190638\\\\\n",
    "\\end{align}\n",
    "\n",
    "These roots were found and tested to be within the selected tolerance of $10^{-10}$.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "![](https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **derivative** of a function of a **single** variable is meaure of the rate at which the output of the function is changing with respect to the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Wiki_slope_in_2d.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a linear function, this is the classic **rise over run** and when written \n",
    "\n",
    "$$f(x) = \\beta_1x+\\beta_0$$\n",
    "\n",
    "the derivative of $f(x)$ is $\\beta_1$ and is written\n",
    "\n",
    "$$f'(x) = \\beta_1$$\n",
    "\n",
    "This is also called **slope**.\n",
    "\n",
    "For more complicated functions, the derivative takes on a more complicated form. \n",
    "\n",
    "Earlier, we estimated the derivative using the **difference quotient**. \n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "Rather than finding our derivatives analytically, we are going to continue to use this estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h=1E-10):\n",
    "    return float((f(x + h) - f(x)) / h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The derivative of a constant is zero.\n",
    "\n",
    "If $f(x)=a$ where $a\\in\\mathbb{R}$ i.e. $a$ is a number\n",
    "\n",
    "$$f'(x)=0$$\n",
    "\n",
    "This is very intuitive. The derivative is a rate of change. If a function is constant, it is not changing, therefore the rate of change is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Derivative of a Multi-Variable Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bad News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of the functions with which we will be working will not be functions of single variables.\n",
    "\n",
    "Consider the following function\n",
    "\n",
    "$$f(x,y) = x^2 + y^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "independent_values = np.arange(-1.1, 1.1, 0.1)\n",
    "X, Y = np.meshgrid(independent_values,\n",
    "                   independent_values)\n",
    "\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, \n",
    "                       rstride=1, \n",
    "                       cstride=1, \n",
    "                       cmap='YlOrRd')\n",
    "ax.view_init(30, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can no longer think of the derivative of this function as before. \n",
    "\n",
    "The function itself evaluates to a single value, in this case the $z$-axis or height of the plot above.\n",
    "\n",
    "But it is a function of two variables. If we think of moving along the surface, we can move in two directions.\n",
    "\n",
    "Our derivative is going to need a value in each direction. \n",
    "\n",
    "The derivative of $f(x,y)$ is going to be a vector with two values. \n",
    "\n",
    "The vector of these values, one for each variable in the function, is the **Gradient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Partial Derivative of a Multi-Variable Function\n",
    "\n",
    "The partial derivative of a multi-variable function is the derivative of the function with respect to a single variable **with all other variables held constant**.\n",
    "\n",
    "Consider the bowl,\n",
    "\n",
    "$$f(x,y) = x^2 + y^2$$\n",
    "\n",
    "To find the partial derivative of $f$ with respect to $x$, we hold $y$ constant.\n",
    "\n",
    "$$\\partial f_x = 2x$$\n",
    "\n",
    "We lose the $y$ term because $y$ is constant with respect to $x$ and the derivative of a constant is zero.\n",
    "\n",
    "By the same token \n",
    "\n",
    "$$\\partial f_y = 2y$$\n",
    "\n",
    "Armed with these two partial derivatives, we have our gradient\n",
    "\n",
    "$$\\nabla f = (2x, 2y)$$\n",
    "\n",
    "$\\nabla$ is a symbol called \"nabla\" and means \"the gradient of\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our function and treat its inputs as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda (x, y): x**2 + y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  a partial difference quotient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h=10E-10):\n",
    "    # add h to just the ith element of v\n",
    "    w = [v_j + (h if j == i else 0) \n",
    "         for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad(f, v, h=10E-10):\n",
    "    return [partial_difference_quotient(f, v, i)\n",
    "            for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_grad(f, v):\n",
    "    this_grad = grad(f, v)\n",
    "    plt.arrow(v[0], v[1], \n",
    "              this_grad[0]-v[0], this_grad[1]-v[1],\n",
    "              linewidth=0.1,\n",
    "              head_width=0.1,\n",
    "              head_length=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print grad(f, (0,.2))\n",
    "print grad(f, (.1,.12))\n",
    "print grad(f, (.3,.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.arange(-3,3,.1)\n",
    "Y = np.arange(-3,3,.1)\n",
    "Z = [[x_i**2 + y_i**2 for x_i in X] for y_i in X]\n",
    "plt.pcolormesh(X, Y, Z, cmap='YlOrRd')\n",
    "plt.colorbar() #need a colorbar to show the intensity scale\n",
    "\n",
    "for x_i in [-1,-0.5,0,0.5,1]:\n",
    "    for y_i in [-1,-0.5,0,0.5,1]:\n",
    "        plot_grad(f, (x_i, y_i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the Gradient\n",
    "\n",
    "1. The direction of the gradient is the direction of the steepest slope at a given point i.e. the direction of the greatest increase of a function.\n",
    "1. The steepness of the slope at that point is the magnitude (or length) of the gradient vector.\n",
    "1. The opposite of the gradient i.e. $-\\nabla f$ will point in the direction of the greatest decrease of a function.\n",
    "   - another way to say this is that $f(a)$ will decrease the fastest at $a$ in the direction of the negative gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Field of Gradients\n",
    "\n",
    "It is helpful to visualize the gradient of a function as a field of gradients. \n",
    "\n",
    "Here we visualize a field of negative gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/grad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can imagine dropping a marble onto this surface. In an ideal situation, it will trace a path described by the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/gradient-descent.png)\n",
    "\n",
    "We can use this property of the gradient to our advantage when attempting to minimize a function, by simply following the path of greatest descent from every point at which we stand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of a Function via Gradient Descent\n",
    "\n",
    "### Domain and Data\n",
    "\n",
    "This problem comes from the field of optimization. We will typically be given a multi-variable function. Here we consider a bowl, \n",
    "\n",
    "$$f(x,y) = x^2 + y^2$$\n",
    "\n",
    "Henceforth, we will refer to $x$ and $y$ as $x_1$ and $x_2$ and together they are\n",
    "\n",
    "$$\\mathbf x = (x_1, x_2)$$\n",
    "\n",
    "with\n",
    "\n",
    "$$f(\\mathbf x) = x_1^2 + x_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda v: sum([x_i**2 for x_i in v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem Statement\n",
    "We seek a local minimum to a multi-variable function, which is to say, we seek the values of our input variable that will yield a minimal value of our function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Statement\n",
    "\n",
    "In simple cases, the minimum of a function can be found by setting the derivative equal to zero and solving analytically.\n",
    "\n",
    "To solve this problem, we use an iterative optimization algorithm known as the Gradient Descent. \n",
    "\n",
    "_Gradient Descent_\n",
    "1. (Optional) Visualize the function\n",
    "2. Choose a learning rate for the descent e.g. $\\gamma=0.1$ \n",
    "3. Randomly select an initial vector value for our function e.g. \n",
    "\n",
    "$$\\mathbf x_0 = (.332,.562)$$\n",
    "   \n",
    "4. Find the gradient of the $f$ at $\\mathbf x$ i.e. $\\nabla f(\\mathbf{x}_0)$\n",
    "\n",
    "5. $\\mathbf{x}_1$ will be a better approximation to a minima, where $\\mathbf{x}_1$ is\n",
    "\n",
    "   $$\\mathbf{x}_1 = \\mathbf{x}_0 - \\gamma \\nabla f(\\mathbf{x}_0)$$\n",
    "   \n",
    "4. Repeat until a sufficiently accurate value is reached.\n",
    "\n",
    "   $$\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\gamma \\nabla f(\\mathbf{x}_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(f, v, h=10E-10):\n",
    "    return np.array([partial_difference_quotient(f, v, i)\n",
    "                     for i, _ in enumerate(v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_iteration(f, v, gamma=0.1):\n",
    "    return v - gamma*grad(f,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "\n",
    "To measure the performance of our descent we will examine the difference between successive steps. When the difference falls below a certain threshold, we can consider the descent to have settled into a minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_magnitude(gradient):\n",
    "    return np.sqrt(sum([x_i**2 for x_i in gradient]))\n",
    "    \n",
    "def grad_precision(grad_1, grad_2):\n",
    "    return np.abs(grad_magnitude(grad_2)-\n",
    "                  grad_magnitude(grad_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "We will set this difference at $10^{-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_1 = np.arange(-3,3,.1)\n",
    "X_2 = np.arange(-3,3,.1)\n",
    "F_X = [[x_1_i**2 + x_2_i**2 \n",
    "        for x_1_i in X_1] \n",
    "       for x_2_i in X_2]\n",
    "\n",
    "plt.pcolormesh(X_1, X_2, F_X, cmap='YlOrRd')\n",
    "plt.colorbar() #need a colorbar to show the intensity scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_0 = np.array((.332,.562))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_1 = gradient_descent_iteration(f, x_0)\n",
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_1 = np.arange(-.7,.7,.1)\n",
    "X_2 = np.arange(-.7,.7,.1)\n",
    "F_X = [[x_1_i**2 + x_2_i**2 \n",
    "        for x_1_i in X_1] \n",
    "       for x_2_i in X_2]\n",
    "\n",
    "plt.pcolormesh(X_1, X_2, F_X, cmap='YlOrRd')\n",
    "plt.plot(x_0[0], x_0[1], '*')\n",
    "plt.plot(x_1[0], x_1[1], '*')\n",
    "\n",
    "plt.colorbar() #need a colorbar to show the intensity scale\n",
    "plt.ylim(-.6,.6)\n",
    "plt.xlim(-.6,.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Five More Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_est = x_1\n",
    "for i in range(5):\n",
    "    x_est = gradient_descent_iteration(f, x_est)\n",
    "    print x_est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterations & Gradient Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_est = np.array((.332,.562))\n",
    "gd_precs = []\n",
    "for i in range(20):\n",
    "    last_x_est = x_est\n",
    "    x_est = gradient_descent_iteration(f, x_est)\n",
    "    gd_prec = grad_precision(last_x_est, x_est)\n",
    "    gd_precs.append(gd_prec)\n",
    "    print \"{:30} {:0.10f} \".format(x_est, gd_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Gradient Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(6), gd_precs, label='gradient descent precisions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_est = np.array((.332,.562))\n",
    "x_ests = [x_est]\n",
    "gd_precs = []\n",
    "for i in range(13):\n",
    "    last_x_est = x_est\n",
    "    x_est = gradient_descent_iteration(f, x_est)\n",
    "    x_ests.append(x_est)\n",
    "    gd_prec = grad_precision(last_x_est, x_est)\n",
    "    gd_precs.append(gd_prec)\n",
    "    print \"{:30} {:0.10f} \".format(x_est, gd_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_1 = np.arange(-.7,.7,.1)\n",
    "X_2 = np.arange(-.7,.7,.1)\n",
    "F_X = [[x_1_i**2 + x_2_i**2 \n",
    "        for x_1_i in X_1] \n",
    "       for x_2_i in X_2]\n",
    "\n",
    "plt.pcolormesh(X_1, X_2, F_X, cmap='YlOrRd')\n",
    "\n",
    "for x in x_ests:\n",
    "    plt.plot(x[0], x[1], '*')\n",
    "\n",
    "plt.colorbar() #need a colorbar to show the intensity scale\n",
    "plt.ylim(-.6,.6)\n",
    "plt.xlim(-.6,.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
