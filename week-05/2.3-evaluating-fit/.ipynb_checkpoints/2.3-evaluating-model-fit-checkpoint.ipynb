{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Evaluating Model Fit\n",
    "Week 5 Lesson 2.3\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Understand the fundamentals of evaluating classifiers\n",
    "- Understand precision, recall, accuracy, and f1-score\n",
    "- Know how to use sklearn.metrics functions to easily compute these metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain & Data\n",
    "\n",
    "### Domain\n",
    "\n",
    "Prepared for the Neural Information Processing Symposium 2003 Feature Extraction Workshop\n",
    "\n",
    "http://clopinet.com/isabelle/Projects/NIPS2003\n",
    "\n",
    "### Data \n",
    "\n",
    "MADELON is an artificial dataset, which was part of the NIPS 2003 feature selection challenge. This is a two-class classification problem with continuous input variables. The difficulty is that the problem is multivariate and highly non-linear\n",
    "\n",
    "MADELON is an artificial dataset containing data points grouped in 32 clusters placed on the vertices of a five dimensional hypercube and randomly labeled +1 or -1. The five dimensions constitute 5 informative features. 15 linear combinations of those features were added to form a set of 20 (redundant) informative features. Based on those 20 features one must separate the examples into the 2 classes (corresponding to the +-1 labels). We added a number of distractor feature called 'probes' having no predictive power. The order of the features and patterns were randomized. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The NIPS 2003 challenge in feature selection is to find feature selection algorithms that significantly outperform methods using all features in performing a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Statement\n",
    "\n",
    "We will develop a binary classification model using a K Nearest Neighbors classifier.\n",
    "\n",
    "\n",
    "## Metric \n",
    "\n",
    "Today, we are largely exploring the dataset. We will use \n",
    "the default metric included with the classifier.\n",
    "\n",
    "## Benchmark\n",
    "\n",
    "We will be assessing after our work today what an appropriate benchmark might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import chdir; chdir('../lib')\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from madelon import load_madelon_set_into_df\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "madelon_feature_df, madelon_target_df = load_madelon_set_into_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Key metrics (5 mins)\n",
    "\n",
    "Classification problems and models are evaluated differently than regression models. Whereas regression models predict a continuous variable, classification problems predict probability of belonging to a class of outcome.\n",
    "\n",
    "Instead of evaluating models based on error like in regression, **we evaluate the models based on the correct and incorrect labeling of classes**.\n",
    "\n",
    "Most classification metrics are based on four outcome categories during prediction:\n",
    "\n",
    "- **True Positives:** A positive class observation (1) is correctly classified as positive by the model.\n",
    "- **False Positive:** A negative class observation (0) is incorrectly classified as positive.\n",
    "- **True Negative:** A negative class observation is correctly classified as negative.\n",
    "- **False Negative:** A positive class observation is incorrectly classified as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_classification(model, X, y, model_args,random_state_split,):\n",
    "    X_train,     \\\n",
    "        X_test,  \\\n",
    "        y_train, \\\n",
    "        y_test = train_test_split(X, y, \n",
    "                                  random_state=random_state_split)\n",
    "\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    this_model = model(**model_args)\n",
    "    this_model.fit(X_train, y_train)\n",
    "    \n",
    "    training_predictions = this_model.predict(X_train)\n",
    "    testing_predictions = this_model.predict(X_test)\n",
    "    \n",
    "    this_model_class_name = this_model.__class__.__name__\n",
    "    this_model_args = ' '.join([str(key)+':'+str(val) \n",
    "                     for key,val in model_args.items()])\n",
    "    \n",
    "    print(\"{} {}\".format(this_model_class_name,\n",
    "                         this_model_args))\n",
    "    \n",
    "    return {'model': this_model,\n",
    "            'y_train_pred' : training_predictions,\n",
    "            'y_test_pred' : testing_predictions,\n",
    "            'X_test' : X_test,\n",
    "            'X_train' : X_train,\n",
    "            'y_test' : y_test,\n",
    "            'y_train' : y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_knn = standard_classification(KNeighborsClassifier,\n",
    "                                      madelon_feature_df,\n",
    "                                      madelon_target_df,\n",
    "                                      {'n_neighbors' : 17,\n",
    "                                       'n_jobs':-1},\n",
    "                                      random_state_split=42)\n",
    "\n",
    "trained_logreg = standard_classification(LogisticRegression,\n",
    "                                         madelon_feature_df,\n",
    "                                         madelon_target_df,\n",
    "                                         {'random_state' : 17,\n",
    "                                          'n_jobs':-1},\n",
    "                                         random_state_split=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification evaluation metric fundamentals\n",
    "\n",
    "##### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labeled_confusion_matrix(trained_model):\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(trained_model['y_test'], \n",
    "                                             trained_model['y_test_pred']))\n",
    "    conf_mat.index = [str(cls)+'_act' \n",
    "                      for cls in trained_model['model'].classes_]\n",
    "\n",
    "    conf_mat.columns = [str(cls)+'_pred' \n",
    "                      for cls in trained_model['model'].classes_]\n",
    "\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is very basic, and while may not seem that useful contains all of the information required for calculating more complex evaluation metrics.\n",
    "\n",
    "A confusion matrix has as rows and columns the classes modeled by your classifier. In the case of logistic regression this will be a 2x2 matrix. Rows indicate the actual class, and columns indicate the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_confusion_matrix(trained_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_confusion_matrix(trained_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above:\n",
    "  \n",
    "- 141 of the negatives were correctly identified\n",
    "- 131 of the positives were correctly identified\n",
    "- 108 negatives were classified as positive\n",
    "- 120 positives were classified as negative\n",
    "\n",
    "\n",
    "  From the 2-variable confusion matrix we can calculate **true positives**, **false positives**, **true negatives**, and **false negatives** directly from the cells. Tuning your model to adjust these metrics depends on your priorities. In healthcare for example, you may want to minimize false negatives at the expense of more false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent Practice\n",
    "\n",
    "Update the method below to return true postives, false positives, true negatives, false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labeled_confusion_matrix(trained_model):\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(trained_model['y_test'], \n",
    "                                             trained_model['y_test_pred']))\n",
    "    conf_mat.index = [str(cls)+'_act' \n",
    "                      for cls in trained_model['model'].classes_]\n",
    "\n",
    "    conf_mat.columns = [str(cls)+'_pred' \n",
    "                      for cls in trained_model['model'].classes_]\n",
    "\n",
    "    try:\n",
    "        return conf_mat, true_postives, false_positives, true_negatives, false_negatives\n",
    "    except NameError:\n",
    "        print(\"Looks like you have some work to do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm, tp, fp, tn, fn = labeled_confusion_matrix(trained_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "  Accuracy is simply the *proportion of classes correctly predicted by the model*.\n",
    "\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{True Positive} + \\text{True Negatives}}{Total}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idependent Practice\n",
    "\n",
    "Complete the following \"roll your own\" method for calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def my_accuracy(trained_model):\n",
    "    return None\n",
    "\n",
    "assert my_accuracy(trained_knn) == accuracy_score(trained_knn['y_test'],\n",
    "                                                  trained_knn['y_test_pred']), \\\n",
    "         'Those are not the same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "*Precision* is the ability of the classifier to *avoid mislabeling when the observation belongs in another class.*\n",
    "\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "\n",
    "  A precision score of 1 indicates that the classifier never mistakenly added observations from another class. A precision score of 0 would mean that the classifier misclassified every instance of the current class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idependent Practice\n",
    "\n",
    "Complete the following \"roll your own\" method for calculating precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def my_precision(trained_model):\n",
    "    return None\n",
    "\n",
    "assert my_precision(trained_knn) == precision_score(trained_knn['y_test'],\n",
    "                                                    trained_knn['y_test_pred']), \\\n",
    "         'Those are not the same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "*recall* is the ability of the classifier to correctly identify all observations in the current class.\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "A recall score of 1 indicates that the classifier correctly predicted (found) all observations of the current class (by implication, no false negatives, or misclassifications of the current class). A recall score of 0 alternatively means that the classifier missed all observations of the current class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idependent Practice\n",
    "\n",
    "Complete the following \"roll your own\" method for calculating recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def my_recall(trained_model):\n",
    "    return None\n",
    "\n",
    "assert my_recall(trained_knn) == recall_score(trained_knn['y_test'],\n",
    "                                              trained_knn['y_test_pred']), \\\n",
    "         'Those are not the same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "\n",
    "**f1-score** is the harmonic mean of the precision and recall. The harmonic mean is used here rather than the more conventional arithmetic mean because the harmonic mean is more appropriate for averaging rates.\n",
    "\n",
    "The f1-score's best value is 1 and worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once.\n",
    "\n",
    "$$\\text{F}1\\text{-Score} = 2\\cdot\\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "**support** is simply the number of observations of the labelled class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idependent Practice\n",
    "\n",
    "Complete the following \"roll your own\" method for calculating F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def my_f1_score(trained_model):\n",
    "    return None\n",
    "\n",
    "assert my_f1_score(trained_knn) == f1_score(trained_knn['y_test'],\n",
    "                                            trained_knn['y_test_pred']), \\\n",
    "         'Those are not the same'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
